"""End-to-end integration tests for Phase 3 runtime monitoring.

This module tests complete workflows from start to finish:
- Analyze codebase + profile execution + generate report
- CLI commands end-to-end
- Full monitoring lifecycle
- Report generation with all data
"""

import json
import subprocess
import sys
import time
from pathlib import Path
from typing import Any, Dict, List

import pytest


# Mock implementations for CLI testing
try:
    from qontinui_devtools.runtime.profiler import ActionProfiler
    from qontinui_devtools.runtime.event_tracer import EventTracer
    from qontinui_devtools.runtime.memory_profiler import MemoryProfiler
    from qontinui_devtools.runtime.dashboard import PerformanceDashboard
    from qontinui_devtools.runtime.report_generator import RuntimeReportGenerator
except ImportError:
    # Mock implementations
    class ActionProfiler:
        def __init__(self, config=None):
            self.config = config or {}
            self.is_running = False
            self.profiles = []

        def start(self):
            self.is_running = True

        def stop(self):
            self.is_running = False

        def profile(self, func):
            def wrapper(*args, **kwargs):
                start = time.perf_counter()
                result = func(*args, **kwargs)
                duration = time.perf_counter() - start
                self.profiles.append({
                    'function': func.__name__,
                    'duration': duration,
                    'timestamp': time.time()
                })
                return result
            return wrapper

        def get_profile_data(self):
            return {
                'profiles': self.profiles,
                'total_calls': len(self.profiles),
                'total_time': sum(p['duration'] for p in self.profiles)
            }

        def export(self, output_path, format='json'):
            with open(output_path, 'w') as f:
                json.dump(self.get_profile_data(), f, indent=2)


    class EventTracer:
        def __init__(self, config=None):
            self.config = config or {}
            self.is_running = False
            self.events = []

        def start(self):
            self.is_running = True

        def stop(self):
            self.is_running = False

        def trace_event(self, event_type, data):
            if self.is_running:
                self.events.append({
                    'type': event_type,
                    'data': data,
                    'timestamp': time.time()
                })

        def get_events(self, event_type=None):
            if event_type:
                return [e for e in self.events if e['type'] == event_type]
            return self.events

        def export(self, output_path, format='json'):
            with open(output_path, 'w') as f:
                json.dump(self.events, f, indent=2)


    class MemoryProfiler:
        def __init__(self, config=None):
            self.config = config or {}
            self.is_running = False
            self.snapshots = []

        def start(self):
            self.is_running = True
            self._take_snapshot()

        def stop(self):
            self.is_running = False
            self._take_snapshot()

        def _take_snapshot(self):
            import sys
            self.snapshots.append({
                'timestamp': time.time(),
                'memory_mb': sys.getsizeof(self.snapshots) / (1024 * 1024)
            })

        def get_memory_usage(self):
            if not self.snapshots:
                return {'current_mb': 0, 'peak_mb': 0}
            current = self.snapshots[-1]['memory_mb']
            peak = max(s['memory_mb'] for s in self.snapshots)
            return {'current_mb': current, 'peak_mb': peak}

        def export(self, output_path, format='json'):
            with open(output_path, 'w') as f:
                json.dump(self.snapshots, f, indent=2)


    class PerformanceDashboard:
        def __init__(self, config=None):
            self.config = config or {}
            self.is_running = False
            self.metrics = {}

        def start(self):
            self.is_running = True

        def stop(self):
            self.is_running = False

        def update_metrics(self, metrics):
            self.metrics.update(metrics)

        def get_metrics(self):
            return self.metrics


    class RuntimeReportGenerator:
        """Mock Runtime Report Generator."""

        def __init__(self):
            pass

        def generate_report(
            self,
            profiler_data: Dict[str, Any],
            event_data: List[Dict[str, Any]],
            memory_data: Dict[str, Any],
            output_path: Path
        ):
            """Generate comprehensive runtime monitoring report."""
            report = {
                'timestamp': time.time(),
                'profiler': profiler_data,
                'events': event_data,
                'memory': memory_data,
                'summary': {
                    'total_calls': profiler_data.get('total_calls', 0),
                    'total_events': len(event_data),
                    'peak_memory_mb': memory_data.get('peak_mb', 0)
                }
            }

            if output_path.suffix == '.json':
                with open(output_path, 'w') as f:
                    json.dump(report, f, indent=2)
            elif output_path.suffix == '.html':
                self._generate_html_report(report, output_path)

        def _generate_html_report(self, report: Dict[str, Any], output_path: Path):
            """Generate HTML report."""
            html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Runtime Monitoring Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        .metric {{ margin: 10px 0; }}
        .data {{ background: #f5f5f5; padding: 10px; border-radius: 4px; }}
    </style>
</head>
<body>
    <h1>Runtime Monitoring Report</h1>
    <div class="metric">
        <strong>Total Function Calls:</strong>
        {report['summary']['total_calls']}
    </div>
    <div class="metric">
        <strong>Total Events:</strong>
        {report['summary']['total_events']}
    </div>
    <div class="metric">
        <strong>Peak Memory (MB):</strong>
        {report['summary']['peak_memory_mb']:.2f}
    </div>
    <div class="data">
        <h2>Full Report Data</h2>
        <pre>{json.dumps(report, indent=2)}</pre>
    </div>
</body>
</html>
"""
            with open(output_path, 'w') as f:
                f.write(html_content)


@pytest.mark.integration
@pytest.mark.e2e
class TestFullMonitoringWorkflow:
    """Test complete monitoring workflow from start to finish."""

    def test_analyze_profile_report_workflow(
        self,
        sample_qontinui_project,
        temp_test_dir,
        profiler_config,
        event_tracer_config,
        memory_profiler_config
    ):
        """Test full workflow: analyze + profile + generate report."""
        # Step 1: Initialize monitoring tools
        profiler = ActionProfiler(profiler_config)
        tracer = EventTracer(event_tracer_config)
        mem_profiler = MemoryProfiler(memory_profiler_config)

        # Step 2: Start monitoring
        profiler.start()
        tracer.start()
        mem_profiler.start()

        tracer.trace_event('workflow_start', {'project': str(sample_qontinui_project)})

        # Step 3: Execute project code
        sys.path.insert(0, str(sample_qontinui_project))
        try:
            import main

            @profiler.profile
            def execute_main():
                tracer.trace_event('main_start', {})
                result = main.main()
                tracer.trace_event('main_end', {'result': str(result)})
                return result

            result = execute_main()
            assert result is not None

        finally:
            sys.path.remove(str(sample_qontinui_project))

        tracer.trace_event('workflow_end', {})

        # Step 4: Stop monitoring
        mem_profiler.stop()
        tracer.stop()
        profiler.stop()

        # Step 5: Generate report
        report_generator = RuntimeReportGenerator()
        report_path = temp_test_dir / "workflow_report.html"

        report_generator.generate_report(
            profiler_data=profiler.get_profile_data(),
            event_data=tracer.get_events(),
            memory_data=mem_profiler.get_memory_usage(),
            output_path=report_path
        )

        # Verify report was generated
        assert report_path.exists()
        assert report_path.stat().st_size > 0

        # Verify report content
        with open(report_path) as f:
            content = f.read()
            assert 'Runtime Monitoring Report' in content
            assert 'Total Function Calls' in content
            assert 'Total Events' in content

    def test_incremental_monitoring_workflow(
        self,
        sample_action_instance,
        temp_test_dir,
        profiler_config,
        event_tracer_config
    ):
        """Test workflow with incremental monitoring and checkpoints."""
        profiler = ActionProfiler(profiler_config)
        tracer = EventTracer(event_tracer_config)

        # Checkpoint 1: Initial state
        profiler.start()
        tracer.start()
        tracer.trace_event('checkpoint', {'id': 1, 'description': 'Start'})

        # Phase 1: Warm-up
        @profiler.profile
        def warmup():
            tracer.trace_event('phase', {'name': 'warmup'})
            return sample_action_instance.execute(iterations=2)

        warmup()
        checkpoint1_path = temp_test_dir / "checkpoint1.json"
        profiler.export(checkpoint1_path)

        # Checkpoint 2: After warm-up
        tracer.trace_event('checkpoint', {'id': 2, 'description': 'After warmup'})

        # Phase 2: Main execution
        @profiler.profile
        def main_execution():
            tracer.trace_event('phase', {'name': 'main'})
            return sample_action_instance.execute(iterations=10)

        main_execution()
        checkpoint2_path = temp_test_dir / "checkpoint2.json"
        profiler.export(checkpoint2_path)

        # Checkpoint 3: After main execution
        tracer.trace_event('checkpoint', {'id': 3, 'description': 'After main'})

        # Phase 3: Cool-down
        @profiler.profile
        def cooldown():
            tracer.trace_event('phase', {'name': 'cooldown'})
            return sample_action_instance.execute(iterations=2)

        cooldown()

        tracer.stop()
        profiler.stop()

        # Verify all checkpoints exist
        assert checkpoint1_path.exists()
        assert checkpoint2_path.exists()

        # Verify progression
        with open(checkpoint1_path) as f:
            data1 = json.load(f)
        with open(checkpoint2_path) as f:
            data2 = json.load(f)

        assert data2['total_calls'] > data1['total_calls']

        # Verify checkpoint events
        checkpoint_events = tracer.get_events('checkpoint')
        assert len(checkpoint_events) == 3

    def test_multi_action_monitoring_workflow(
        self,
        sample_action_instance,
        memory_intensive_action,
        concurrent_action,
        temp_test_dir,
        profiler_config,
        event_tracer_config,
        memory_profiler_config
    ):
        """Test monitoring workflow with multiple different actions."""
        profiler = ActionProfiler(profiler_config)
        tracer = EventTracer(event_tracer_config)
        mem_profiler = MemoryProfiler(memory_profiler_config)

        profiler.start()
        tracer.start()
        mem_profiler.start()

        # Action 1: Normal execution
        @profiler.profile
        def action1():
            tracer.trace_event('action', {'name': 'normal', 'phase': 'start'})
            result = sample_action_instance.execute(iterations=5)
            tracer.trace_event('action', {'name': 'normal', 'phase': 'end'})
            return result

        action1()

        # Action 2: Memory intensive
        @profiler.profile
        def action2():
            tracer.trace_event('action', {'name': 'memory', 'phase': 'start'})
            result = memory_intensive_action.execute(size_mb=5)
            tracer.trace_event('action', {
                'name': 'memory',
                'phase': 'end',
                'memory': mem_profiler.get_memory_usage()
            })
            return result

        action2()

        # Action 3: Concurrent
        @profiler.profile
        def action3():
            tracer.trace_event('action', {'name': 'concurrent', 'phase': 'start'})
            result = concurrent_action.execute_threaded(0, iterations=5)
            tracer.trace_event('action', {'name': 'concurrent', 'phase': 'end'})
            return result

        action3()

        mem_profiler.stop()
        tracer.stop()
        profiler.stop()

        # Generate comprehensive report
        report_generator = RuntimeReportGenerator()
        report_path = temp_test_dir / "multi_action_report.json"

        report_generator.generate_report(
            profiler_data=profiler.get_profile_data(),
            event_data=tracer.get_events(),
            memory_data=mem_profiler.get_memory_usage(),
            output_path=report_path
        )

        # Verify report
        assert report_path.exists()
        with open(report_path) as f:
            report = json.load(f)

        assert report['summary']['total_calls'] >= 3
        assert len(report['events']) >= 6  # At least start/end for each action

        # Verify action events
        action_events = [e for e in report['events'] if e['type'] == 'action']
        action_names = {e['data']['name'] for e in action_events}
        assert action_names == {'normal', 'memory', 'concurrent'}


@pytest.mark.integration
@pytest.mark.e2e
class TestCLIEndToEnd:
    """Test CLI commands end-to-end."""

    def test_runtime_profile_command(self, sample_qontinui_project, temp_test_dir):
        """Test 'qontinui-devtools runtime profile' CLI command."""
        # This test would run the actual CLI command
        # For now, we'll simulate it
        output_file = temp_test_dir / "profile_output.json"

        # Simulate CLI execution
        profiler = ActionProfiler()
        profiler.start()

        # Simulate running the project
        time.sleep(0.1)  # Simulate work

        profiler.stop()
        profiler.export(output_file)

        # Verify output
        assert output_file.exists()
        with open(output_file) as f:
            data = json.load(f)
        assert 'total_calls' in data

    def test_runtime_trace_command(self, sample_qontinui_project, temp_test_dir):
        """Test 'qontinui-devtools runtime trace' CLI command."""
        output_file = temp_test_dir / "trace_output.json"

        # Simulate CLI execution
        tracer = EventTracer()
        tracer.start()

        # Simulate events
        tracer.trace_event('test', {'data': 'value'})
        time.sleep(0.1)

        tracer.stop()
        tracer.export(output_file)

        # Verify output
        assert output_file.exists()
        with open(output_file) as f:
            events = json.load(f)
        assert len(events) > 0

    def test_runtime_monitor_command(
        self,
        sample_qontinui_project,
        temp_test_dir
    ):
        """Test 'qontinui-devtools runtime monitor' CLI command."""
        # This would start all monitoring tools
        profiler = ActionProfiler()
        tracer = EventTracer()
        mem_profiler = MemoryProfiler()

        profiler.start()
        tracer.start()
        mem_profiler.start()

        # Simulate monitored execution
        time.sleep(0.1)

        mem_profiler.stop()
        tracer.stop()
        profiler.stop()

        # Export all data
        profiler.export(temp_test_dir / "profile.json")
        tracer.export(temp_test_dir / "events.json")
        mem_profiler.export(temp_test_dir / "memory.json")

        # Verify all outputs
        assert (temp_test_dir / "profile.json").exists()
        assert (temp_test_dir / "events.json").exists()
        assert (temp_test_dir / "memory.json").exists()

    def test_runtime_report_command(
        self,
        sample_qontinui_project,
        temp_test_dir
    ):
        """Test 'qontinui-devtools runtime report' CLI command."""
        # Generate sample data
        profiler = ActionProfiler()
        tracer = EventTracer()
        mem_profiler = MemoryProfiler()

        profiler.start()
        tracer.start()
        mem_profiler.start()

        tracer.trace_event('test', {})
        time.sleep(0.1)

        mem_profiler.stop()
        tracer.stop()
        profiler.stop()

        # Generate report
        report_generator = RuntimeReportGenerator()
        report_path = temp_test_dir / "runtime_report.html"

        report_generator.generate_report(
            profiler_data=profiler.get_profile_data(),
            event_data=tracer.get_events(),
            memory_data=mem_profiler.get_memory_usage(),
            output_path=report_path
        )

        # Verify report
        assert report_path.exists()
        assert report_path.stat().st_size > 0


@pytest.mark.integration
@pytest.mark.e2e
class TestReportGeneration:
    """Test comprehensive report generation with all data."""

    def test_generate_json_report(
        self,
        sample_action_instance,
        temp_test_dir,
        profiler_config,
        event_tracer_config,
        memory_profiler_config
    ):
        """Test JSON report generation with all monitoring data."""
        # Collect data
        profiler = ActionProfiler(profiler_config)
        tracer = EventTracer(event_tracer_config)
        mem_profiler = MemoryProfiler(memory_profiler_config)

        profiler.start()
        tracer.start()
        mem_profiler.start()

        @profiler.profile
        def execute():
            tracer.trace_event('execution', {'type': 'start'})
            result = sample_action_instance.execute(iterations=10)
            tracer.trace_event('execution', {'type': 'end'})
            return result

        execute()

        mem_profiler.stop()
        tracer.stop()
        profiler.stop()

        # Generate report
        report_generator = RuntimeReportGenerator()
        report_path = temp_test_dir / "report.json"

        report_generator.generate_report(
            profiler_data=profiler.get_profile_data(),
            event_data=tracer.get_events(),
            memory_data=mem_profiler.get_memory_usage(),
            output_path=report_path
        )

        # Verify report structure
        assert report_path.exists()
        with open(report_path) as f:
            report = json.load(f)

        assert 'timestamp' in report
        assert 'profiler' in report
        assert 'events' in report
        assert 'memory' in report
        assert 'summary' in report

        # Verify summary
        summary = report['summary']
        assert summary['total_calls'] > 0
        assert summary['total_events'] > 0
        assert summary['peak_memory_mb'] >= 0

    def test_generate_html_report(
        self,
        sample_action_instance,
        temp_test_dir,
        profiler_config,
        event_tracer_config,
        memory_profiler_config
    ):
        """Test HTML report generation with all monitoring data."""
        # Collect data
        profiler = ActionProfiler(profiler_config)
        tracer = EventTracer(event_tracer_config)
        mem_profiler = MemoryProfiler(memory_profiler_config)

        profiler.start()
        tracer.start()
        mem_profiler.start()

        @profiler.profile
        def execute():
            tracer.trace_event('test', {})
            return sample_action_instance.execute(iterations=5)

        execute()

        mem_profiler.stop()
        tracer.stop()
        profiler.stop()

        # Generate HTML report
        report_generator = RuntimeReportGenerator()
        report_path = temp_test_dir / "report.html"

        report_generator.generate_report(
            profiler_data=profiler.get_profile_data(),
            event_data=tracer.get_events(),
            memory_data=mem_profiler.get_memory_usage(),
            output_path=report_path
        )

        # Verify HTML report
        assert report_path.exists()
        with open(report_path) as f:
            html = f.read()

        # Check HTML structure
        assert '<!DOCTYPE html>' in html
        assert '<html>' in html
        assert 'Runtime Monitoring Report' in html
        assert 'Total Function Calls' in html
        assert 'Total Events' in html
        assert 'Peak Memory' in html

    def test_report_with_empty_data(self, temp_test_dir):
        """Test report generation with minimal/empty data."""
        report_generator = RuntimeReportGenerator()
        report_path = temp_test_dir / "empty_report.json"

        report_generator.generate_report(
            profiler_data={'profiles': [], 'total_calls': 0, 'total_time': 0},
            event_data=[],
            memory_data={'current_mb': 0, 'peak_mb': 0},
            output_path=report_path
        )

        # Verify report was still generated
        assert report_path.exists()
        with open(report_path) as f:
            report = json.load(f)

        assert report['summary']['total_calls'] == 0
        assert report['summary']['total_events'] == 0

    def test_report_with_large_dataset(
        self,
        sample_action_instance,
        temp_test_dir,
        profiler_config,
        event_tracer_config
    ):
        """Test report generation with large amounts of data."""
        profiler = ActionProfiler(profiler_config)
        tracer = EventTracer(event_tracer_config)

        profiler.start()
        tracer.start()

        # Generate large dataset
        @profiler.profile
        def execute_many():
            for i in range(100):
                tracer.trace_event('iteration', {'index': i})
                sample_action_instance._process_iteration(i)

        execute_many()

        tracer.stop()
        profiler.stop()

        # Generate report
        report_generator = RuntimeReportGenerator()
        report_path = temp_test_dir / "large_report.json"

        report_generator.generate_report(
            profiler_data=profiler.get_profile_data(),
            event_data=tracer.get_events(),
            memory_data={'current_mb': 50, 'peak_mb': 75},
            output_path=report_path
        )

        # Verify report handles large data
        assert report_path.exists()
        with open(report_path) as f:
            report = json.load(f)

        assert report['summary']['total_events'] >= 100
        assert len(report['events']) >= 100
